{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem statement:\n",
    "\n",
    "Amazon is argubly the largest direct-to-consumer marketplace in the world.  The site itself houses over millions of products by hundreds of thousands of businesses selling.  Being a site that sells such a large array of products, the categories in which Amazon markets its products is also in the thousands.  These categories are key attributes for placing one listing over another listing.  (i.e. if a user is trying to search for a fork, amazon will list those listings that are listed in the Kitchen items category that have a high sales volume with high ratings on the top.  Instead of any listing with a keyword fork that has higher sales listings.)      \n",
    "\n",
    "Trying to scrap Amazon data is extremely difficult since this is against Amazon's TOUs and also Amazon has placed many security measures from anyone attempting to do so.  However a web application called Jungle Scout is the leading analytics service that most Amazon sellers rely on when looking to do product research as well as forecast sales.  \n",
    "\n",
    "Another obstacle is that there are many items on Amazon that are sold for off-label uses.  That is, the category and use that is indiciated is not the true intention of the product.  The reason for this is because Amazon has certain policies that restrict the sale of such goods, but some of these policies may not be strictly enforced.  A question for this dataset would be if we can identify categories correctly with product descriptions and if we can identify \"grey-categories\" in which Amazon currently has not labeled yet.      \n",
    "\n",
    "My goal is to scrap the JungleScout data to find out more about the Amazon Marketplace as a whole. Even though this data is based on estimates created by the Junglescout Accusales algorithm, for this project and purpose; I will be relying on the estimates to gain a general understanding of the Amazon Marketplace data.  Another question would be, what kind factors of a listing generate the sales and if a model can be used to approximate sales totals. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Processing:\n",
    "\n",
    "I created a datascrapper to scrap data from the JungleScout database.  There are 64 million products in the database, so I will subset my data to contain only certain categories and listings where there are also unit sales above 100 per month.   After scrapping the data from JungleScout, I would need to create a table to store the data as well as format the data to the correct object types.  \n",
    "\n",
    "### Additional Data Features: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Analysis:\n",
    "\n",
    "Next to dig deeper into the data:\n",
    "\n",
    "- A look at the distribution of the sales data shows that the data is severely skewed towards sales below 2500.  We should separate the data to include only sales within 2500 sales per month. \n",
    "\n",
    "![title](img/SalesDist.png)\n",
    "\n",
    "- When we look at the distribution for sales under 2500, we are still seeing a very long tail.  The next step is to look at sales under 1000.\n",
    "\n",
    "![title](img/SalesDist2500.png)\n",
    "\n",
    "- Looking at the distribution for sales under 1000, our tail is getting smaller.  We do notice there is a cut-off at around 180 sales.  This is due to the filter built-in into JungleScout that we are not seeing data below this.  \n",
    "\n",
    "![title](img/SalesDist1000.png)\n",
    "\n",
    "- Looking across the different categories where sales were below 1000 units per month.  It appears that there are not many categories that are drastically different in terms of mean sales from one to another.  \n",
    "\n",
    "![title](img/Boxplot1000.png)\n",
    "\n",
    "- Looking at the different categories and the number of listings available in each category. \n",
    "\n",
    "- By looking at only sales that are under 1000, we still have over 91% of our original data with 18,987 datapoints.  We should continue to build our model around this subset of data to get avoid influences by large outliers.  I also removed rankings above 300,000 for the same reason.  The following pairplot is a pairplot of all features from where sales are below 1000 and rankings are below 300,000.\n",
    "\n",
    "![title](img/pplot.png)\n",
    "\n",
    "- To make things more clear, we look at the correlation matrix of the dataset.  Intuitively, this correlation makes logical sense.  We see that when rankings are lower, we see higher sales.  We also see that Listing Quality also leads to a higher number of sales.  Ratings and Number of Ratings are also positively correlated.  An interesting piece of information is that, the number of ratings has a much higher correlation factor to Sales than the actual ratings.  Looking at the pairplot above, there does appear to be a limit to this.  For instance, we do not see very many high sales numbers where ratings are below 3.  However, where ratings are 3.5 to 5, it appears that the sales are much closer together. \n",
    "\n",
    "![title](img/CorrPlot.png)\n",
    "\n",
    "- Next we perform a PCA to attempt to reduce the features.  From our PCA analysis, it looks like we can model our data with 7 features insead of the 8.  However, since the features are not extremely large we can use the entire dataset since the reduction of features is not very large. We can feed this to our model.  \n",
    "\n",
    "![title](img/FeatureVariance.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
